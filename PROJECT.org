#+TITLE: Building a dynamic, executable and discoverable network of interlinked software and data

Software is central in scientific research engineering. But more
importantly than in other engineering disciplines, the RSE products
live and evolve in an highly dynamic and cross-disciplinary
environment, resulting in risks to fade out very quickly and
opportunities to cross initial domain boundaries. This project aims at
developing infrastructure to support some major contemporary
challenges of modern data-driven science, i.e. and set up an open
digital information infrastructure using automation and
standardisation as primary force to enable broader and more open
dissemination, application, impact and creativity of research software
engineering outputs

Modern science relies on ever increasing quantities of data (becoming
more plentiful, bigger, different and researchers implement ways to
interrogate it - some have coined it the data deluge) and on
trustworthy and sustainable software to analyse, understand and
interpret them. Each of these pillars of modern scientific activities
are generally developed and maintained independently. On one hand, a
typical data pipeline starts with a data producer submitting these
annotated data to an official repository that, after some basic
curation, serves it to the wider community. This flow is
unidirectional and restricted to published (or soon to be published)
data and, once part of the official repository, becomes inert (updates
and further annotations are very rare) and, most often, dormant. On
the other hand, the exposure of a research software is often limited
by the application (or applicability) to a reduced set of known data
sets, and hence limited to the original team or
sub-community. Surprisingly, while each data/software information and
development streams can't be conceived in isolation, and their true
potential emerges from their interoperation, little effort exist to
facilitate streamlined interactions and discoverability. The ideal
infrastructure would enable transparent integration of different data
source (local or remote, public or private), data sets (transparent to
file formats) through data APIs and software architectures (local or
remote) through software APIs/pipelines. The novel potential, other
than re-use of data and software and potential for automation, the
novelty is discoverability of data and software leading to new
applications and use cases (from data and software perspectives) and
contributing these back as novel, publicly available data/software
meta-analyses.


A recent project involved developing, implementing and applying a
transfer learning algorithm that learns from different data sources to
classify features. In our specific question, the classification is
aimed at inferring the spatial sub-cellular localisation of proteins
using experimental data (primary data source) and gene ontology
annotation data (auxiliary data source). Our algorithm was initially
inspired from a leaves classification problem (using bio-metrical
measurement from fresh and dried specimens) and was improved and
adapted for spatial proteomics use cases. We would like to apply it to
another domain, genomics data being an obvious candidate. How should
we proceed?  Look for something relevant in the literature, ask
colleagues, ... none of these approaches are effective and
substantially slow cross-fertilisation between domains and high rates
of duplication. 

The transition from the first publication of the algorithm in the
International Conference on Machine Learning (ICML) and leaves
biometry data to an updated implementation (Biocondutor pRoloc
software package) and application to 5 proteomics data sets initially
relied on a fortuitous and fortunate discovery. What further
transitions and improvements can be anticipated?  If the immediate
transition are prohibitively slow or difficult, the dissemination,
improvement and generalisation of the software is dramatically
limited. The project's goal is to offer systematic ways to enable such
transitions.


#+CAPTION: A simple software/data evolution example
[[./Figures/data-algo-transitions.png]]
[[./Figures/data-algo-transitions-2.png]]

*** Meta-analyses

Testing software with more data results in more reliable
software. Testing the software with diverse data results in more use
cases and more generally applicable software. Exploring a data set
with different (easily discoverable) software enable better
understanding of the data.


The rational of this project is that to make data (and meta-data) more
interesting, it should be complemented by a diverse set descriptive
obtained by different software platforms. Conversely, to make
software more interesting to a diverse audience, it should be applied
to a breath of data sets across various domains. 


#+CAPTION: Infrastructure overview
[[./Figures/overview_20150529_150535.jpg]]

Such a platform should enable 

- data and software annotation and interoperation;
- data and software software discoverability;
- data and software re-usability;
- enhanced data and software dissemination;
- emergence of novel data and software use cases;

for all members of the scientific community and public, irrespective
of technical IT skills, knowledge of data formats and data generation
infrastructure, and data/software provenance. 


The domain of high-throughput biology, and more specifically
proteomics will be used as a use case to develop and illustrate the
proposed infrastructure.

** More notes

Example illustrating the difficulty of cross talking between software
and data domains: existence of multiple file formats, annotation and
data provenance, which considerably hinder a systematic exploitation
of data for any or multiple software for expert users (i.e. requires
then to design infrastructure 

The aim of this project is to design and develop an infrastructure
that would serve the scientist communities with data- and
software-as-a-service infrastructures that support transparent cross
talk between data and software.



Building up from 'bare-bone' open source software (some of which I
have developed over the years and maintain) up to transparent on-line
infrastructure, that would be adequate for collaborations with
industry (I have some contacts) and/or commercialisation.


In a nutshell (and open to discussion), I would like to expand the
existing infrastructure for the analysis of high-throughput biology
data (using proteomics and integration of proteomics and genomics as a
use case)


What about taverna? galaxi? AnnotationHub? We want to make use of
this. 

** Also, 

- mining of academic repos, scientific literature, software repositories
- publishing platforms to disseminate new software/data/analysis associations
- support for citing and disseminating data/software/papers/meta-analysis
- dissemination through social media

** References

- [[https://peerj.com/articles/cs-1/][Achieving human and machine accessibility of cited data in scholarly publications]]
- [[http://blogs.lse.ac.uk/impactofsocialsciences/2015/05/28/elseviers-non-sharing-policy-barbour/][Elsevier's new sharing policy is really a reversal of the rights of authors]]
- [[http://datafairport.org/][Data FAIRport - Find, Access, Interoperate & Re-use Data]]
- [[https://scicrunch.org/resources][Research Resources ID]]


