###  Name and contact details

Laurent Gatto  
Computational Proteomics Unit  
Cambridge Systems Biology Centre  
Department of Biochemistry  
University of Cambridge  
Tennis Court Road  
Cambridge, CB2 1GA, UK  
Email: lg390@cam.ac.uk  
Tel: +44 (0) 1223 760253

### A summary of your career to date

After a PhD in Evolutionary Genetics and complementary studies in
Computer Science, I worked for 3 years in Industry with a focus on
computational pipelines R&D. In 2010, I moved to the University of
Cambridge, where I concentrated on data and software infrastructure
for quantitative and spatial proteomics, first as a Post-doctoral
Research Associate and later as a Senior Research Associate. In recent
years, I have been awarded an Software Sustainability Institute
fellowship and have become a Software Carpentry instructor.

### A brief explanation of why you feel the RSE Fellowship is appropriate for you

Since the completion of my PhD, the emphasis of my work has focused on
delivering software solutions to tackle specific data-driven
scientific questions, that have culminated in several state-of-the-art
software, data and educational packages. The extend to which I can
deliver research software is limited by the constraint academic
setting and the limited opportunities for focused research software
engineering practice that is required to tackle more substantial
software projects. The fellowship will give me a unique opportunity to
tackle wide-ranging cross-disciplinary challenges in data-driven
science and research software engineering, such as dissemination,
discoverability and novel uses of software and hence promoting
collaborations and maximising impact of RSEs.

### A brief summary of what you would hope to achieve from an RSE Fellowship

Software is central in scientific research engineering. But more
importantly than in other engineering disciplines, the RSE products
live and evolve in an highly dynamic and cross-disciplinary
environment, resulting in risks to fade out very quickly and
opportunities to cross initial domain boundaries. This project aims at
developing infrastructure to support some major contemporary
challenges of modern data-driven science, i.e. set up an open digital
information infrastructure using automation and standardisation as a
primary force to enable broader and more open dissemination,
application, impact and creativity of research software engineering
outputs.

Modern science relies on ever increasing quantities of data (becoming
more plentiful, bigger, different and researchers implement new ways
to interrogate it - some have coined it the 'data deluge') and on
trustworthy and sustainable software to analyse, understand and
interpret them. Each of these pillars of modern scientific activities
are generally developed and maintained independently. On one hand, a
typical data pipeline starts with a data producer submitting these
annotated artefacts to an official repository that, after some basic
curation, serves them to the wider community. This flow is
unidirectional and restricted to published (or soon to be published)
data and, once part of the official repository, becomes inert (updates
and further annotations are very rare) and, most often, dormant. On
the other hand, the exposure of a research software is often limited
by the application (or applicability) to a reduced set of known data
sets, and hence limited to the original team or
sub-community. Surprisingly, while each data/software information and
development streams can't be conceived in isolation, and their true
potential emerges from their interoperation, little efforts exist to
facilitate streamlined interactions and discoverability and enable new
data/software meta-analyses.

A recent project involved developing, implementing and applying a
transfer learning algorithm that learns from different data sources to
classify features. In our specific question, the classification is
aimed at inferring the spatial sub-cellular localisation of proteins
using experimental data (primary data source) and gene ontology
annotation data (auxiliary data source). Our algorithm was initially
inspired from a leaves classification problem (using morphological
measurement from fresh and dried specimens) and was improved and
adapted for spatial proteomics use cases. We would like to apply it to
another domain, genomics data being an obvious candidate; however,
manual exploration and reliance on personal contacts are not effective
and substantially slow cross-fertilisation between domains and result
in high rates of duplication. The transition from the first
publication of the algorithm in the International Conference on
Machine Learning and leaves morphology data to a functional
implementation and application to five diverse proteomics data sets
initially relied on a fortuitous and fortunate discovery. What further
transitions and improvements could be anticipated?  If the immediate
transition are prohibitively slow or difficult, the dissemination,
improvement and generalisation of the software is dramatically
limited. The project's goal is to offer systematic ways to enable such
transitions.

Concretely, the first major milestone of the project is a set of
interconnected databases of experimental data, software and
publications, all spanning different disciplines, created from mining
relevant repositories. The relations between records within and
between these databases will be browseable and searchable through an
online portal and programmatically accessible through specific
APIs. Given specific properties of existing records, or new,
user-defined properties describing software, data or publications, new
matches can then be identified. Properties related to quality and
suistainability of software and data (metadata, documentation,
testing, ...) will be used to favour the promotion of more
suistainable code. Known cross-disciplinary data/software outputs will
be used to optimise and assess the relevance of the
infrastructure. The second major milestone aims at facilitating the
application of the relations identified above by enabling transparent
integration of different data source (local or remote, public or
private), data sets (transparent to file formats) through data APIs
and software architectures (local or remote) through software
APIs/pipelines. Milestone one enables systematic exploration of RSE
entities and milestone two supports their automated and standardised
meta-analysis. Each of these deliverables will be complemented by
dedicated open communication channels to openly publicise and
disseminate the new software/data relations and use cases.



