<!-- Interested parties should seek advice from their desired host -->
<!-- institution in the first instance, since EPSRC requires the host to -->
<!-- provide significant support and commitment to the RSE Fellow. -->

<!-- Candidates who consider they have the appropriate profile of skills -->
<!-- and experience are invited to send an ‘Intent to Submit’ note to -->
<!-- edward.clarke@epsrc.ac.uk by 16:00 on 12 June 2015.  The note should -->
<!-- be no more than 1 page A4 and include: -->

###  Name and contact details

Laurent Gatto  
Computational Proteomics Unit  
Cambridge Systems Biology Centre
Department of Biochemistry  
University of Cambridge  
Tennis Court Road  
Cambridge, CB2 1GA, UK  
Email: lg390@cam.ac.uk  
Tel: +44 (0) 1223 760253

### A summary of your career to date

After a PhD in Evolutionary Biology and complementary studies in
Computer Science, I worked for 3 years in Industry with a focus on
computational pipelines R&D. In 2010, I moved to the University of
Cambridge, where I concentrated on data and software infrastructure
for quantitative and spatial proteomics, first as a Post-doctoral
Research Associate and later as a Senior Research Associate. In recent
years, I have been awarded an Software Sustainability Institute
fellowship and have become a Software Carpentry instructor.

### A brief explanation of why you feel the RSE Fellowship is appropriate for you

Since the completion of my PhD, the emphasis of my work has focused on
delivering software solutions to tackle specific data-driven
scientific questions, that have culminated in several state-of-the-art
software, data and educational packages. The extend to which I can
deliver research software is limited by the constraint academic
setting and the limited opportunities for focused research software
engineering practice that is required to tackle more substantial
software projects. The fellowship will give me a unique opportunity to
tackle wide-ranging challenges in data-driven science and research
software engineering.

### A brief summary of what you would hope to achieve from an RSE Fellowship

Software is central in scientific research engineering. But more
importantly than in other engineering disciplines, the RSE products
live and evolve in an highly dynamic and cross-disciplinary
environment, resulting in risks to fade out very quickly and
opportunities to cross initial domain boundaries. This project aims at
developing infrastructure to support some major contemporary
challenges of modern data-driven science, i.e. set up an open digital
information infrastructure using automation and standardisation as a
primary force to enable broader and more open dissemination,
application, impact and creativity of research software engineering
outputs.

Modern science relies on ever increasing quantities of data (becoming
more plentiful, bigger, different and researchers implement new ways
to interrogate it - some have coined it the 'data deluge') and on
trustworthy and sustainable software to analyse, understand and
interpret them. Each of these pillars of modern scientific activities
are generally developed and maintained independently. On one hand, a
typical data pipeline starts with a data producer submitting these
annotated data to an official repository that, after some basic
curation, serves it to the wider community. This flow is
unidirectional and restricted to published (or soon to be published)
data and, once part of the official repository, becomes inert (updates
and further annotations are very rare) and, most often, dormant. On
the other hand, the exposure of a research software is often limited
by the application (or applicability) to a reduced set of known data
sets, and hence limited to the original team or
sub-community. Surprisingly, while each data/software information and
development streams can't be conceived in isolation, and their true
potential emerges from their interoperation, little efforts exist to
facilitate streamlined interactions and discoverability. The ideal
infrastructure would enable transparent integration of different data
source (local or remote, public or private), data sets (transparent to
file formats) through data APIs and software architectures (local or
remote) through software APIs/pipelines. The novel potential, other
than re-use of data and software and potential for automation, is
discoverability of data and software leading to new applications and
use cases (from data and software perspectives) and contributing these
back as novel, publicly available data/software meta-analyses.

A recent project involved developing, implementing and applying a
transfer learning algorithm that learns from different data sources to
classify features. In our specific question, the classification is
aimed at inferring the spatial sub-cellular localisation of proteins
using experimental data (primary data source) and gene ontology
annotation data (auxiliary data source). Our algorithm was initially
inspired from a leaves classification problem (using morphological
measurement from fresh and dried specimens) and was improved and
adapted for spatial proteomics use cases. We would like to apply it to
another domain, genomics data being an obvious candidate; however,
manual exploration and reliance on personal contacts are not effective
and substantially slow cross-fertilisation between domains and high
rates of duplication.

The transition from the first publication of the algorithm in the
International Conference on Machine Learning and leaves morphology
data to a functional implementation and application to five diverse
proteomics data sets initially relied on a fortuitous and fortunate
discovery. What further transitions and improvements could be
anticipated?  If the immediate transition are prohibitively slow or
difficult, the dissemination, improvement and generalisation of the
software is dramatically limited. The project's goal is to offer
systematic ways to enable such transitions.

